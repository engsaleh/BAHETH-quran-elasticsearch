# quran_search_backend.py (Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø©)
# ÙŠÙˆÙØ± ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Elasticsearch ÙˆØ§Ù„ÙÙ‡Ø±Ø³Ø© ÙˆØ§Ù„Ø¨Ø­Ø« Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª

import os
import re
import pandas as pd
from dotenv import load_dotenv
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from index_mapping import mappings
import warnings
import urllib3

# ØªØ¬Ø§Ù‡Ù„ ØªØ­Ø°ÙŠØ±Ø§Øª SSL Ù„Ù„ØªØ·ÙˆÙŠØ±
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore')

# ========================= ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª =========================
load_dotenv()

ES_URL = os.getenv("ES_URL", "https://localhost:9200")
ES_USER = os.getenv("ES_USER", "elastic")
ES_PASS = os.getenv("ES_PASS", "")
INDEX_NAME = os.getenv("ES_INDEX", "quran_search_v1")

# ========================= Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Elasticsearch =========================
es = None
try:
    es = Elasticsearch(
        ES_URL,
        basic_auth=(ES_USER, ES_PASS),
        verify_certs=False,  # âš ï¸ Ù„Ù„ØªØ·ÙˆÙŠØ± ÙÙ‚Ø·
        request_timeout=30,
        max_retries=3,
        retry_on_timeout=True
    )
    if not es.ping():
        raise ValueError("âŒ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Elasticsearch ÙØ´Ù„.")
    print("âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Elasticsearch Ø¨Ù†Ø¬Ø§Ø­.")
except Exception as e:
    print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Elasticsearch: {str(e)}")
    es = None

# ========================= Ù†Ù…ÙˆØ°Ø¬ Embeddings =========================
model = None
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

try:
    # Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„
    model = SentenceTransformer(MODEL_NAME)
    print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings: {MODEL_NAME}")
except Exception as e:
    print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings: {str(e)}")
    print("ğŸ’¡ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„...")
    try:
        model = SentenceTransformer("all-MiniLM-L6-v2")
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø¯ÙŠÙ„")
    except:
        print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Embeddings. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­.")

# ========================= Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© =========================

def remove_arabic_diacritics(text):
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„Ø£Ù„Ù Ø§Ù„Ø®Ù†Ø¬Ø±ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    if not text:
        return ""
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[\u064b-\u0652\u0670]', '', text)
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù„Ù Ø§Ù„Ø®Ù†Ø¬Ø±ÙŠØ©
    text = text.replace('\u0670', '')
    return text.strip()

def normalize_arabic_text(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª ÙˆØ§Ù„Ø£Ù„ÙØ§Øª)"""
    if not text:
        return ""

    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
    text = re.sub('[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    text = re.sub('Ù‰', 'ÙŠ', text)
    text = re.sub('Ø©', 'Ù‡', text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = remove_arabic_diacritics(text)

    return text

def get_embedding(text, normalize=True):
    """ØªÙˆÙ„ÙŠØ¯ embedding Ù„Ù„Ù†Øµ"""
    if model is None:
        return []
    if not text:
        return []

    try:
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
        if normalize:
            text = normalize_arabic_text(text)
        text = text.replace("\n", " ").strip()

        # ØªÙˆÙ„ÙŠØ¯ embedding
        embedding = model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ embedding: {str(e)}")
        return []

# ========================= Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ =========================

def create_quran_index(delete_if_exists=True):
    """Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Elasticsearch"""
    if es is None:
        print("âš ï¸ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ØªØµØ§Ù„.")
        return False

    try:
        # Ø­Ø°Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if delete_if_exists and es.indices.exists(index=INDEX_NAME):
            es.indices.delete(index=INDEX_NAME)
            print(f"ğŸ”„ ØªÙ… Ø­Ø°Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù‚Ø¯ÙŠÙ… '{INDEX_NAME}'.")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¬Ø¯ÙŠØ¯
        es.indices.create(index=INDEX_NAME, body=mappings)
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ '{INDEX_NAME}' Ø¨Ù†Ø¬Ø§Ø­.")
        return True

    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: {str(e)}")
        return False

# ========================= Ø§Ù„ÙÙ‡Ø±Ø³Ø© =========================

def index_quran_data(file_path="quran_data.csv", batch_size=500):
    """ÙÙ‡Ø±Ø³Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù† Ù…Ø¹ progress bar"""
    if es is None:
        print("âš ï¸ Ù„Ø§ ÙŠÙ…ÙƒÙ† ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§ØªØµØ§Ù„.")
        return False

    if not os.path.exists(file_path):
        print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return False

    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data = pd.read_csv(file_path)
        total_verses = len(data)
        print(f"ğŸ“š Ø¬Ø§Ø±ÙŠ ÙÙ‡Ø±Ø³Ø© {total_verses} Ø¢ÙŠØ©...")

        actions = []
        processed = 0

        for idx, row in data.iterrows():
            doc_id = f"{row['sura']}-{row['aya']}"
            aya_text = str(row["text"])

            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
            source_doc = {
                "sura": int(row["sura"]),
                "aya": int(row["aya"]),
                "text": aya_text,
                "text_raw": remove_arabic_diacritics(aya_text),
                "embedding": get_embedding(aya_text)
            }

            # Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ„ Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
            if "sura_name" in row and pd.notna(row["sura_name"]):
                source_doc["sura_name"] = str(row["sura_name"])
            if "translation_en" in row and pd.notna(row["translation_en"]):
                source_doc["translation_en"] = str(row["translation_en"])

            actions.append({
                "_index": INDEX_NAME,
                "_id": doc_id,
                "_source": source_doc
            })

            processed += 1

            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
            if processed % 100 == 0:
                progress = (processed / total_verses) * 100
                print(f"â³ Ø§Ù„ØªÙ‚Ø¯Ù…: {processed}/{total_verses} ({progress:.1f}%)")

            # Ø¥Ø±Ø³Ø§Ù„ Ø¯ÙØ¹Ø©
            if len(actions) >= batch_size:
                helpers.bulk(es, actions, chunk_size=batch_size)
                actions = []

        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        if actions:
            helpers.bulk(es, actions, chunk_size=batch_size)

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙÙ‡Ø±Ø³
        es.indices.refresh(index=INDEX_NAME)

        print(f"âœ… ØªÙ… ÙÙ‡Ø±Ø³Ø© {total_verses} Ø¢ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
        return True

    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³Ø©: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

# ========================= Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¨Ø­Ø« =========================

def lexical_search(query, top_k=10, fields=None, fuzziness="AUTO"):
    """Ø¨Ø­Ø« Ù„ØºÙˆÙŠ Ù…Ø­Ø³Ù‘Ù†"""
    if es is None:
        return []

    if fields is None:
        fields = ["text^2", "text.raw^1.5", "sura_name^1.2", "translation_en"]

    try:
        response = es.search(
            index=INDEX_NAME,
            body={
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": fields,
                        "fuzziness": fuzziness,
                        "type": "best_fields",
                        "operator": "or",
                        "minimum_should_match": "75%"
                    }
                },
                "size": top_k,
                "_source": ["sura", "aya", "text", "sura_name"]
            }
        )
        return response["hits"]["hits"]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ØºÙˆÙŠ: {str(e)}")
        return []

def semantic_search_knn(query, top_k=10, num_candidates=None):
    """Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… kNN"""
    if es is None or model is None:
        return []

    if num_candidates is None:
        num_candidates = min(top_k * 10, 500)

    query_vec = get_embedding(query)
    if not query_vec:
        return []

    try:
        response = es.search(
            index=INDEX_NAME,
            knn={
                "field": "embedding",
                "query_vector": query_vec,
                "k": top_k,
                "num_candidates": num_candidates
            },
            size=top_k,
            _source=["sura", "aya", "text", "sura_name"]
        )
        return response["hits"]["hits"]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ kNN: {str(e)}")
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ script_score
        return semantic_search_script_score(query, top_k)

def semantic_search_script_score(query, top_k=10):
    """Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… script_score (Ø§Ø­ØªÙŠØ§Ø·ÙŠ)"""
    if es is None or model is None:
        return []

    query_vec = get_embedding(query)
    if not query_vec:
        return []

    try:
        response = es.search(
            index=INDEX_NAME,
            body={
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vec, 'embedding') + 1.0",
                            "params": {"query_vec": query_vec}
                        }
                    }
                },
                "size": top_k,
                "_source": ["sura", "aya", "text", "sura_name"]
            }
        )
        return response["hits"]["hits"]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ script_score: {str(e)}")
        return []

def hybrid_search(query, top_k=10, lex_weight=0.5, sem_weight=0.5):
    """Ø¨Ø­Ø« Ù‡Ø¬ÙŠÙ† Ù…Ø­Ø³Ù‘Ù† ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù„ØºÙˆÙŠ ÙˆØ§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
    if es is None:
        return []

    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ÙƒÙ„Ø§ Ø§Ù„Ù†ÙˆØ¹ÙŠÙ†
    lexical_results = lexical_search(query, top_k=top_k * 2)

    if model is not None:
        semantic_results = semantic_search_knn(query, top_k=top_k * 2)
    else:
        semantic_results = []

    # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
    combined_scores = {}

    # Ø¥Ø¶Ø§ÙØ© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù„ØºÙˆÙŠ
    for hit in lexical_results:
        doc_id = hit['_id']
        score = hit['_score'] * lex_weight
        combined_scores[doc_id] = {
            'score': score,
            'hit': hit
        }

    # Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    for hit in semantic_results:
        doc_id = hit['_id']
        score = hit['_score'] * sem_weight

        if doc_id in combined_scores:
            combined_scores[doc_id]['score'] += score
        else:
            combined_scores[doc_id] = {
                'score': score,
                'hit': hit
            }

    # ØªØ±ØªÙŠØ¨ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    sorted_results = sorted(
        combined_scores.items(),
        key=lambda x: x[1]['score'],
        reverse=True
    )[:top_k]

    # Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    final_results = []
    for doc_id, data in sorted_results:
        hit = data['hit'].copy()
        hit['_score'] = data['score']
        final_results.append(hit)

    return final_results

# ========================= Ø¯ÙˆØ§Ù„ Ø¥Ø¶Ø§ÙÙŠØ© =========================

def search_by_sura(sura_number, top_k=50):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø³ÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©"""
    if es is None:
        return []

    try:
        response = es.search(
            index=INDEX_NAME,
            body={
                "query": {
                    "term": {"sura": sura_number}
                },
                "sort": [{"aya": "asc"}],
                "size": top_k
            }
        )
        return response["hits"]["hits"]
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø³ÙˆØ±Ø©: {str(e)}")
        return []

def get_verse(sura, aya):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©"""
    if es is None:
        return None

    doc_id = f"{sura}-{aya}"
    try:
        response = es.get(index=INDEX_NAME, id=doc_id)
        return response['_source']
    except:
        return None

def get_index_stats():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙÙ‡Ø±Ø³"""
    if es is None:
        return None

    try:
        count = es.count(index=INDEX_NAME)
        stats = es.indices.stats(index=INDEX_NAME)

        return {
            'total_verses': count['count'],
            'index_size': stats['indices'][INDEX_NAME]['total']['store']['size_in_bytes'],
            'index_name': INDEX_NAME
        }
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {str(e)}")
        return None
